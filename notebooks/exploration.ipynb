{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Exploration__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "from datasets import load_dataset\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.linear_model import SGDClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### __Preprocessing__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"AiresPucrs/stanford-encyclopedia-philosophy\", split = 'train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = dataset.to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns=['metadata'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('wordnet')\n",
    "# nltk.download('omw-1.4')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "def preprocess_tokenize(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    tokens = word_tokenize(text)\n",
    "    return tokens\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [token for token in tokens if token not in stop_words]\n",
    "\n",
    "df['tokenized_text'] = df['text'].apply(preprocess_tokenize)\n",
    "df['tokenized_text'] = df['tokenized_text'].apply(remove_stopwords)\n",
    "df['lemmatized_text'] = df['tokenized_text'].apply(lambda tokens: [lemmatizer.lemmatize(token) for token in tokens])\n",
    "df['stemmed_text'] = df['tokenized_text'].apply(lambda tokens: [stemmer.stem(token) for token in tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new column 'final_text' from the 'lemmatized_text' tokens.\n",
    "df['final_text'] = df['lemmatized_text'].apply(lambda tokens: ' '.join(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 3),   # Range of Unigrams to Trigrams.\n",
    "    max_features=15000,   # Limit to 15,000 terms.\n",
    "    min_df=7,             # Ignore terms that appear in fewer than 7 documents.\n",
    "    max_df=0.75,          # Ignore terms that appear in more than 75% of documents.\n",
    "    use_idf=True,\n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X_tfidf = vectorizer.fit_transform(df['final_text'])\n",
    "y = df['category']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_tfidf, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = np.unique(y_train)\n",
    "class_weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weight_dict = dict(zip(classes, class_weights))\n",
    "sample_weight = np.array([class_weight_dict[label] for label in y_train])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = SGDClassifier(\n",
    "    loss='hinge',     # Hinge loss.\n",
    "    max_iter=1,       # One iteration per call to partial_fit.\n",
    "    tol=None,         # Disable internal early stopping.\n",
    "    warm_start=True,  # Retain model state between partial_fit calls.\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "n_epochs = 20\n",
    "for epoch in tqdm.tqdm(range(n_epochs), desc=\"Training epochs\"):\n",
    "    clf.partial_fit(X_train, y_train, sample_weight=sample_weight, classes=classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nMaking predictions on test set...\")\n",
    "y_pred = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "feature_names = np.array(vectorizer.get_feature_names_out())\n",
    "category_to_explain = 'abelard'  # Change as needed.\n",
    "if category_to_explain in clf.classes_:\n",
    "    class_index = list(clf.classes_).index(category_to_explain)\n",
    "    coefficients = clf.coef_[class_index]\n",
    "    top_n = 10\n",
    "    top_features_idx = np.argsort(coefficients)[-top_n:]\n",
    "    top_features = feature_names[top_features_idx]\n",
    "    \n",
    "    print(f\"\\nTop features for category '{category_to_explain}':\")\n",
    "    for i, feature in enumerate(reversed(top_features)):\n",
    "        coef = coefficients[top_features_idx[-(i+1)]]\n",
    "        print(f\"{feature}: {coef:.4f}\")\n",
    "else:\n",
    "    print(f\"Category '{category_to_explain}' not found in the model's classes.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_explanation(input_text):\n",
    "    X_input = vectorizer.transform([input_text])\n",
    "    predicted_category = clf.predict(X_input)[0]\n",
    "    \n",
    "    category_index = list(clf.classes_).index(predicted_category)\n",
    "    coefficients = clf.coef_[category_index]\n",
    "    contributions = X_input.multiply(coefficients).toarray()[0]\n",
    "    \n",
    "    top_n_explain = 5\n",
    "    top_indices = np.argsort(contributions)[-top_n_explain:]\n",
    "    explanation_features = [(feature_names[i], contributions[i]) for i in top_indices if contributions[i] > 0]\n",
    "    \n",
    "    explanation_str = f\"The input was classified as '{predicted_category}' because it contains indicative features such as: \"\n",
    "    explanation_str += \", \".join([f\"{feat} (contribution: {coef:.4f})\" for feat, coef in explanation_features])\n",
    "    return predicted_category, explanation_str\n",
    "\n",
    "example_text = \"In Earth's future, a global crop blight and second Dust Bowl are slowly rendering the planet uninhabitable.\"\n",
    "pred_cat, explanation = predict_with_explanation(example_text)\n",
    "print(\"\\nPrediction and Explanation for example text:\")\n",
    "print(\"Predicted Category:\", pred_cat)\n",
    "print(\"Explanation:\", explanation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paragraph on why the non-deep learning method would not work __(without certain changes in criteria as shown above)__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our attempt to address the problem statement, we found that it was impossible to create viable text generation functionality without the use of a neural network. Initially, we had believed that the use of an ensemble of Hidden Markov Models would be effective. Despite issues in addressing contextual information, it was likely that these models would be able to generate an output that had some level of overlap with the expected output of a prompt. Despite this, the nature of the data proved to be insufficient for this application as it was very large and highly variable. Ultimately, the non‑deep learning methods such as HMMs and n‑gram based models that we implemented lacked the capacity to capture contextual information from long ago, reminiscent of larger context windows. The nuances of language for philosophical texts required coherent text generation. The reliance on local context for these non-deep learning methods meant that they often produced disjointed or repetitive outputs, and they struggled to integrate more complex syntactic structures in the absence of stopwords and pronoun detection. Furthermore, the program was inherently limited by the high dimensionality of traditional bag‑of‑words methods, making it challenging and time consuming to scale to the breadth and depth of a real-world corpus. In contrast, deep neural networks are specifically designed to model sequential and hierarchical language patterns. As such, we implemented a text classification implementation that takes in a sentence and outputs the predicted category with the most important indicative features that led to that conclusion."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
